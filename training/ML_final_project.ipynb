{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsP4a8UfZOyC"
      },
      "outputs": [],
      "source": [
        "# ML Final Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BsM3bfQrG8-0"
      },
      "outputs": [],
      "source": [
        "# import all th required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z-OVYC5FcQlt"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    training_data_path = \"fashion-mnist_train.csv\"\n",
        "    testing_data_path = \"fashion-mnist_test.csv\"\n",
        "\n",
        "    train_csv = pd.read_csv(training_data_path, header=None, skiprows=1)\n",
        "    test_csv = pd.read_csv(testing_data_path, header=None, skiprows=1)\n",
        "\n",
        "    X_train, y_train = train_csv.iloc[:, 1:], train_csv.iloc[:, 0].astype(int)\n",
        "    X_test, y_test = test_csv.iloc[:, 1:], test_csv.iloc[:, 0].astype(int)  # Fixed!\n",
        "\n",
        "    return (np.asarray(X_train)[:10000],\n",
        "            np.asarray(y_train)[:10000],\n",
        "            np.asarray(X_test)[:1000],\n",
        "            np.asarray(y_test)[:1000])\n",
        "    \n",
        "def sigmoid(x):\n",
        "    # Clip to prevent overflow\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CTbMboLScTu-"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BinaryLogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        self.mean = X.mean(axis=0)\n",
        "        self.std = X.std(axis=0) + 1e-8  # Add epsilon to avoid division by zero\n",
        "\n",
        "        X_norm = (X - self.mean) / self.std\n",
        "        n, d = X_norm.shape\n",
        "\n",
        "        # Initialize weights\n",
        "        w = np.zeros(d)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            p = sigmoid(X_norm @ w)\n",
        "            grad = (X_norm.T @ (p - y)) / n\n",
        "            w -= self.lr * grad\n",
        "\n",
        "            #  if gradient is close to zero, stop\n",
        "            if np.linalg.norm(grad) < 1e-6:\n",
        "                break\n",
        "\n",
        "        self.w_ = w\n",
        "        return self\n",
        "\n",
        "    def predict_probability(self, X):\n",
        "        X_norm = (X - self.mean) / self.std\n",
        "        return sigmoid(X_norm @ self.w_)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_probability(X) >= threshold).astype(int)\n",
        "\n",
        "class MulticlassLogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.classifiers_ = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        y = np.asarray(y)\n",
        "        for c in self.classes_:\n",
        "            clf = BinaryLogisticRegression(self.lr, self.epochs)\n",
        "            # Create binary labels\n",
        "            y_binary = (y == c).astype(float)\n",
        "            clf.fit(X, y_binary)\n",
        "            self.classifiers_.append(clf)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        probas = np.stack([clf.predict_probability(X) for clf in self.classifiers_], axis=1)\n",
        "        return self.classes_[np.argmax(probas, axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(z: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Stable softmax\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=np.float64, copy=False)\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    s = e.sum(axis=1, keepdims=True)\n",
        "    s[s == 0.0] = 1.0\n",
        "    return e / s\n",
        "\n",
        "\n",
        "class MulticlassLogisticRegression:\n",
        "    \"\"\"\n",
        "    Multinomial logistic regression trained with batch gradient descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, lr: float = 0.05, epochs: int = 1500, tol: float = 1e-6, l2: float = 1e-4\n",
        "    ):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.tol = tol\n",
        "        self.l2 = l2  # L2 regularization coefficient\n",
        "        self.W_: np.ndarray | None = None  # (n_features, n_classes)\n",
        "        self.classes_: np.ndarray | None = None\n",
        "        self.mean: np.ndarray | None = None  # (n_features,)\n",
        "        self.std: np.ndarray | None = None  # (n_features,)\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) \n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        # Classes and inverse indices (y_idx maps each sample to its class index)\n",
        "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = int(self.classes_.shape[0])\n",
        "\n",
        "        # Standardize with clamped std to avoid exploding features\n",
        "        self.mean = X.mean(axis=0)\n",
        "        std = X.std(axis=0)\n",
        "        std_floor = 1e-2 \n",
        "        self.std = np.where(std < std_floor, std_floor, std)\n",
        "        Xn = (X - self.mean) / self.std\n",
        "\n",
        "        # Initialize weights\n",
        "        self.W_ = np.zeros((n_features, n_classes), dtype=np.float64)\n",
        "\n",
        "        # One-hot labels\n",
        "        Y = np.eye(n_classes, dtype=np.float64)[y_idx]  # (n_samples, n_classes)\n",
        "\n",
        "        # Batch gradient descent with L2\n",
        "        for _ in range(self.epochs):\n",
        "            logits = Xn @ self.W_  # (n_samples, n_classes)\n",
        "            probs = softmax(logits)  # (n_samples, n_classes)\n",
        "            grad = (Xn.T @ (probs - Y)) / n_samples + self.l2 * self.W_\n",
        "\n",
        "            # Update\n",
        "            self.W_ -= self.lr * grad\n",
        "\n",
        "            # Early stopping\n",
        "            if np.linalg.norm(grad) < self.tol:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _standardize(self, X: np.ndarray) -> np.ndarray:\n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        return (X - self.mean) / self.std\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n",
        "        Xn = self._standardize(X)\n",
        "        logits = Xn @ self.W_\n",
        "        if temperature and temperature > 0 and temperature != 1.0:\n",
        "            logits = logits / float(temperature)\n",
        "        return softmax(logits)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)\n",
        "        idx = np.argmax(probs, axis=1)\n",
        "        return self.classes_[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtgeID8zHxO7",
        "outputId": "88dd4a90-e10f-470b-ac40-f6a934407a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8380\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "# Train model\n",
        "model = MulticlassLogisticRegression(lr=0.01, epochs=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# save this model\n",
        "with open(\"fashion-mnist-model.pkl\", 'wb') as f:\n",
        "  dill.dump(model, f)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy4p71R6ZgDk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDomU4X8Tekj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
