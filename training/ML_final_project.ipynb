{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsP4a8UfZOyC"
      },
      "outputs": [],
      "source": [
        "# ML Final Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BsM3bfQrG8-0"
      },
      "outputs": [],
      "source": [
        "# import all th required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z-OVYC5FcQlt"
      },
      "outputs": [],
      "source": [
        "# list of some helper functions\n",
        "def load_data():\n",
        "    training_data_path = \"fashion-mnist_train.csv\"\n",
        "    testing_data_path = \"fashion-mnist_test.csv\"\n",
        "\n",
        "    # preparing train test data\n",
        "    train_csv = pd.read_csv(training_data_path, header=None, skiprows=1)\n",
        "    test_csv = pd.read_csv(testing_data_path, header=None, skiprows=1)\n",
        "    X_train, y_train = train_csv.iloc[:, 1:], train_csv.iloc[:, 0].astype(int)\n",
        "    X_test, y_test = test_csv.iloc[:, 1:], test_csv.iloc[:, 0].astype(int)  \n",
        "\n",
        "    # TODO: need to include all the data later\n",
        "    return (np.asarray(X_train)[:10000],\n",
        "            np.asarray(y_train)[:10000],\n",
        "            np.asarray(X_test)[:1000],\n",
        "            np.asarray(y_test)[:1000])\n",
        "    \n",
        "def sigmoid(x):\n",
        "    # Clip to prevent overflow\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PCA:\n",
        "    def __init__(self, X, num_components=None): #X = input data, num_components = dimensions to keep\n",
        "        self.num_components = num_components\n",
        "\n",
        "        # compute mean of each feature\n",
        "        self.mn = np.mean(X, 0, keepdims=True)\n",
        "\n",
        "        # de-mean the features (subtract mean from each sample)\n",
        "        X_m = X - self.mn\n",
        "\n",
        "        # compute covariance matrix\n",
        "        covmat = X_m.T @ X_m/(len(X)-1)\n",
        "\n",
        "        # compute eigenvalues and eigenvectors\n",
        "        evals,evects = np.linalg.eigh(covmat)\n",
        "\n",
        "        # sort by decreasing variance (largest eigenvalue = direction of max var)\n",
        "        idx = np.argsort(evals)[::-1]\n",
        "        self.evals = evals[idx]\n",
        "        self.evects = evects[:,idx]\n",
        "\n",
        "        # keep only the first n_components if given\n",
        "        if num_components is not None:\n",
        "            self.evals = self.evals[:num_components]\n",
        "            self.evects = self.evects[:, :num_components]\n",
        "\n",
        "    def transform(self,X):\n",
        "        # center the data and project onto the principal components\n",
        "        X_m = X - self.mn\n",
        "        return X_m @ self.evects\n",
        "\n",
        "    def num_effective_dims(self,percvar): # gives number of PCs needed for X% variance\n",
        "        total_var = np.sum(self.evals)\n",
        "        cumulative_vals = 100 * np.cumsum(self.evals) / total_var\n",
        "        i = np.searchsorted(cumulative_vals, percvar)\n",
        "        return i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CTbMboLScTu-"
      },
      "outputs": [],
      "source": [
        "# The one vs rest strategy for multiclass classification  we studied in class is not very efficient for real time analysis.\n",
        "# so, we are not using it. Really slow and only give output but not probabilities.\n",
        "\n",
        "class BinaryLogisticRegression:\n",
        "    \"\"\"Binary Class classification\"\"\"\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.mean = X.mean(axis=0)\n",
        "        self.std = X.std(axis=0) + 1e-8  # Add epsilon to avoid division by zero\n",
        "\n",
        "        X_norm = (X - self.mean) / self.std\n",
        "        n, d = X_norm.shape\n",
        "\n",
        "        # Initialize weights\n",
        "        w = np.zeros(d)\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            p = sigmoid(X_norm @ w)\n",
        "            grad = (X_norm.T @ (p - y)) / n\n",
        "            w -= self.lr * grad\n",
        "\n",
        "            #  if gradient is close to zero, stop\n",
        "            if np.linalg.norm(grad) < 1e-6:\n",
        "                break\n",
        "        self.w_ = w\n",
        "        return self\n",
        "\n",
        "    def predict_probability(self, X):\n",
        "        X_norm = (X - self.mean) / self.std\n",
        "        return sigmoid(X_norm @ self.w_)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_probability(X) >= threshold).astype(int)\n",
        "\n",
        "class MulticlassLogisticRegression:\n",
        "    \"\"\"\n",
        "    use one vs rest startegy for Multiclass classification. \n",
        "    This is slow for real time analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.classifiers_ = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        y = np.asarray(y)\n",
        "        for c in self.classes_:\n",
        "            clf = BinaryLogisticRegression(self.lr, self.epochs)\n",
        "            # Create binary labels\n",
        "            y_binary = (y == c).astype(float)\n",
        "            clf.fit(X, y_binary)\n",
        "            self.classifiers_.append(clf)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        probas = np.stack([clf.predict_probability(X) for clf in self.classifiers_], axis=1)\n",
        "        return self.classes_[np.argmax(probas, axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"Better multiclass classification using softmax regression; Similar to multinomial logistic regression\"\"\"\n",
        "    def __init__(self, lr=0.01, epochs=10000, tol=1e-6):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.tol = tol\n",
        "\n",
        "    def softmax(self, x):\n",
        "        z = x - np.max(x, axis=1, keepdims=True)  \n",
        "        exp_z = np.exp(z)\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        classes, y_idx = np.unique(y, return_inverse=True)\n",
        "        n_classes = len(classes)\n",
        "        self.classes_ = classes\n",
        "\n",
        "        # Normalization\n",
        "        self.mean = X.mean(axis=0)\n",
        "        std = X.std(axis=0)\n",
        "        self.std = np.where(std == 0, 1e-8, std)\n",
        "        X = (X - self.mean) / self.std\n",
        "\n",
        "        # y matrix\n",
        "        Y = np.eye(n_classes, dtype=np.float64)[y_idx]\n",
        "\n",
        "        # Weights\n",
        "        self.W = np.zeros((n_features, n_classes), dtype=np.float64)\n",
        "\n",
        "        # Gradient Descent\n",
        "        for _ in range(self.epochs):\n",
        "            probs = self.softmax((X @ self.W))\n",
        "            grad = (X.T @ (probs - Y)) / n_samples\n",
        "\n",
        "            # updating the weights\n",
        "            self.W -= self.lr * grad\n",
        "\n",
        "            if np.linalg.norm(grad) < self.tol:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _standardize(self, X):\n",
        "        return (X - self.mean) / self.std\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        Xn = self._standardize(X)\n",
        "        return self.softmax((Xn @ self.W))\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        idx = np.argmax(probs, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtgeID8zHxO7",
        "outputId": "88dd4a90-e10f-470b-ac40-f6a934407a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8350\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "# PCA\n",
        "pca = PCA(X_train, num_components=100) # pick # components\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Train model (use better model than one vs many)\n",
        "model = LogisticRegression(lr=0.01, epochs=50000)\n",
        "model.fit(X_train_pca, y_train) # change X_train_pca to X_train for no PCA\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test_pca) # change X_test_pca to X_test for no PCA\n",
        "\n",
        "# save this model to be used later by app.\n",
        "with open(\"fashion-mnist-model.pkl\", 'wb') as f:\n",
        "  pickle.dump(model, f)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gy4p71R6ZgDk"
      },
      "outputs": [],
      "source": [
        "# testing with RBF kernels just to see if we get better accuracy.( The accuracy is even bad)\n",
        "class KMeans:\n",
        "    def __init__(self, n_clusters=50, max_iter=100, tol=1e-4, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X):\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize centers randomly\n",
        "        idx = rng.choice(n_samples, self.n_clusters, replace=False)\n",
        "        centers = X[idx].copy()\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # Assign points to closest center\n",
        "            dists = np.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)\n",
        "            labels = np.argmin(dists, axis=1)\n",
        "\n",
        "            # Compute new centers\n",
        "            new_centers = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) else centers[i] \n",
        "                                    for i in range(self.n_clusters)])\n",
        "            \n",
        "            # Check convergence\n",
        "            if np.linalg.norm(new_centers - centers) < self.tol:\n",
        "                break\n",
        "            centers = new_centers\n",
        "\n",
        "        self.cluster_centers_ = centers\n",
        "        return self\n",
        "\n",
        "class RBFTransformer:\n",
        "    def __init__(self, n_centers=50, sigma=None, random_state=None):\n",
        "        self.n_centers = n_centers\n",
        "        self.sigma = sigma\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Compute centers with simple KMeans\n",
        "        kmeans = KMeans(n_clusters=self.n_centers, random_state=self.random_state)\n",
        "        kmeans.fit(X)\n",
        "        self.centers = kmeans.cluster_centers_\n",
        "\n",
        "        # sigma = median distance to nearest center\n",
        "        dists = np.linalg.norm(X[:, None, :] - self.centers[None, :, :], axis=2)\n",
        "        min_dists = np.min(dists, axis=1)\n",
        "        if self.sigma is None:\n",
        "            self.sigma = np.median(min_dists)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        dists = np.linalg.norm(X[:, None, :] - self.centers[None, :, :], axis=2)\n",
        "        return np.exp(-dists**2 / (2 * self.sigma**2))\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        return self.fit(X).transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bDomU4X8Tekj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RBF Kernel Accuracy: 0.5710\n"
          ]
        }
      ],
      "source": [
        "rbf = RBFTransformer(n_centers=10)\n",
        "X_train_rbf = rbf.fit_transform(X_train)\n",
        "X_test_rbf = rbf.transform(X_test)\n",
        "\n",
        "model_rbf = LogisticRegression(lr=0.01, epochs=1000)\n",
        "model_rbf.fit(X_train_rbf, y_train)\n",
        "y_pred_rbf = model_rbf.predict(X_test_rbf)  \n",
        "\n",
        "accuracy_rbf = np.mean(y_pred_rbf == y_test)\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
